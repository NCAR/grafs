#!/usr/bin/env python
import pandas as pd
import numpy as np
from netCDF4 import Dataset, date2num, stringtochar
from datetime import datetime, timedelta
from pvlib.solarposition import get_solarposition
from pvlib.irradiance import extraradiation
from pvlib.location import Location
import matplotlib.pyplot as plt
from multiprocessing import Pool
import traceback
import argparse
import os


def plot_solar_variability():
    start_date = datetime(1994, 1, 1, 23, 0)
    end_date = datetime(1999,12, 31, 23, 0)
    stations = ["NORM", "SPEN", "MTHE", "BEAV"]
    path = "/idea/mesonet/"
    station_info_file = path + "geoinfo.csv"
    md = MesonetRawData(start_date, end_date, stations, path, station_info_file)
    md.load_data()
    #md.solar_data()
    print md.data["NORM"]["SRAD"]
    print md.running_mean("SRAD", 12, 12)
    print md.running_sd("SRAD", 12, 12)
    md.solar_data()
    clearness_index_mean = md.running_mean("CLRI", 12, 12).dropna()

    clearness_index_sd = md.running_sd("CLRI", 12, 12).dropna()
    def plot_kt_sd_hist(clearness_index_sd, site):
        ci_counts, bx, by = np.histogram2d(clearness_index_sd.index.hour, clearness_index_sd[site],
                                           bins=(np.arange(11.5, 25.5), np.arange(0, 1.0, 0.02)))
        hours = np.arange(13, 24)
        hour_dists = []
        for hour in hours:
            hour_dists.append(clearness_index_sd.loc[clearness_index_sd.index.hour == hour, site])
        plt.figure(figsize=(8, 8))
        plt.pcolormesh(bx[:-1], by[:-1], np.ma.array(ci_counts.T, mask=ci_counts.T == 0), vmin=0,
                       cmap=plt.get_cmap("Reds", 20))
        plt.boxplot(hour_dists, positions=hours)
        plt.xlim(12.5,23.5)
        plt.ylim(0, 0.3)
        plt.colorbar()
        plt.xticks(np.arange(13, 24))
        plt.xlabel("Hour (UTC)")
        plt.ylabel("Clearness Index Standard Deviation")
        plt.title("Diurnal Clearness Index Variability at {0}".format(site))
        plt.savefig("/Users/djgagne2/ci_figures/kt_sd_{0}.png".format(site), dpi=300, bbox_inches="tight")
        plt.close()
    for station in stations:
        print station
        plot_kt_sd_hist(clearness_index_sd, station)
    return


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-s", "--start", required=True, help="Start Date")
    parser.add_argument("-e", "--end", required=True, help="End Date")
    parser.add_argument("-v", "--var", required=True, help="Variables to process")
    parser.add_argument("-m", "--mesopath", required=True, help="Path to Mesonet files")
    parser.add_argument("-p", "--procs", default=1, type=int, help="Number of processors")
    parser.add_argument("-t", "--meta", required=True, help="Path to metadata files")
    parser.add_argument("-o", "--out", required=True, help="Path to output netCDF files")
    args = parser.parse_args()
    variables = args.var.split(",")
    pool = Pool(args.procs)
    dates = pd.DatetimeIndex(start=args.start, end=args.end, freq="1D")
    stations, station_numbers = get_valid_station_data(args.meta)
    print stations
    print station_numbers
    for date in dates.to_pydatetime():
        pool.apply_async(aggregate_mesonet_data, (date - timedelta(hours=1), date + timedelta(hours=23), stations,
                                                  variables, args.mesopath, args.meta + "geoinfo.csv", args.out,
                                                  station_numbers))
    pool.close()
    pool.join()
    return


def get_valid_station_data(meta_path):
    mesonet_meta = pd.read_csv(meta_path + "geoinfo.csv")
    grafs_meta = pd.read_csv(meta_path + "int_obs_sites.asc", sep=";")
    combined = pd.merge(mesonet_meta, grafs_meta, how="left", left_on="stid", right_on="icao")
    combined = combined.dropna()
    return combined["stid"].values, combined["stationNumber"].values


def aggregate_mesonet_data(start_date, end_date, stations, av_variables, path, station_info_file, nc_path,
                           station_numbers, radiation_var="SRAD"):
    try:
        mrd = MesonetRawData(start_date, end_date, stations, path, station_info_file)
        mrd.load_data()
        mrd.solar_data(radiation_var=radiation_var)
        for variable in av_variables:
            mrd.running_mean(variable, 12, 12)
            mrd.running_sd(variable, 12, 12)
        mrd.averaged_data_to_netcdf(nc_path, station_numbers=station_numbers)
    except Exception as e:
        print(traceback.format_exc())
        raise e
    return


class MesonetRawData(object):
    def __init__(self, start_date, end_date, stations, path, station_info_file):
        self.start_date = start_date
        self.end_date = end_date
        self.dates = pd.DatetimeIndex(start=self.start_date, end=self.end_date,
                                      freq="5min")
        self.stations = stations
        self.data = {}
        self.averaged_data = {}
        self.path = path
        self.station_info = pd.read_csv(station_info_file, index_col="stid")
        self.units = dict(RELH="%",
                          TAIR="degrees Celsius",
                          WSPD="m s-1",
                          WVEC="m s-1",
                          WDIR="degrees",
                          WDSD="degrees",
                          WSSD="m s-1",
                          WMAX="m s-1",
                          RAIN="mm",
                          PRES="mb",
                          SRAD="W m-2",
                          TA9M="degrees Celsius",
                          WS2M="m s-1")

    def load_data(self):
        def read_mesonet_day(date):
            filename = self.path + "{0:d}/{1:02d}/{2:02d}/{0:d}{1:02d}{2:02d}{3}.mts".format(
                date.year, date.month, date.day, station.lower())
            data = pd.read_table(filename, sep="[ ]{1,5}", skiprows=2, engine="python",
                                 index_col=False, 
                                 na_values=np.arange(-999,-989, 1).tolist() + ["   "])
            data.index = pd.TimeSeries(pd.Timestamp(date) + pd.TimedeltaIndex(data["TIME"], unit="m"))
            return data
        all_days = np.unique(self.dates.date)
        print "Loading", all_days
        for station in self.stations:
            station_data = pd.concat(map(read_mesonet_day, all_days))
            self.data[station] = station_data.loc[self.dates]

    def running_mean(self, variable, window_size, sample_frequency):
        mean_data = pd.DataFrame()
        for station in self.stations:
            mean_data[station] = pd.rolling_mean(self.data[station][variable], window_size)[
                sample_frequency::sample_frequency]
        self.averaged_data[variable + "_Mean"] = mean_data
        return mean_data

    def running_sd(self, variable, window_size, sample_frequency):
        sd_data = pd.DataFrame()
        for station in self.stations:
            sd_data[station] = pd.rolling_std(self.data[station][variable], window_size)[
                sample_frequency::sample_frequency]
        self.averaged_data[variable + "_SD"] = sd_data
        return sd_data

    def solar_data(self, radiation_var="SRAD"):
        columns = ["elevation", "azimuth", "zenith", "ETRC", "CLRI"]
        for station in self.stations:
            loc = Location(self.station_info.loc[station, "nlat"], self.station_info.loc[station, "elon"], tz="UTC",
                           altitude=self.station_info.loc[station, "elev"])
            solar_data = get_solarposition(self.data[station].index, loc)
            solar_data["EXTR"] = extraradiation(self.data[station].index, method="spencer")
            solar_data["ETRC"] = solar_data["EXTR"] * np.cos(np.radians(solar_data["zenith"]))
            solar_data.loc[solar_data["zenith"] > 90, "ETRC"] = 0
            solar_data["CLRI"] = np.zeros(solar_data["ETRC"].size)
            si = solar_data["ETRC"].values > 0
            solar_data["CLRI"][si] = self.data[station][radiation_var].values[si] / solar_data["ETRC"].values[si]
            solar_data["CLRI"][solar_data["ETRC"] == 0] = np.nan
            self.data[station][columns] = solar_data[columns]

    def averaged_data_to_netcdf(self, out_path, time_units="seconds since 1970-01-01 00 UTC", station_numbers=None):
        print "Saving", self.averaged_data.values()[0].index[0]
        for variable in sorted(self.averaged_data.keys()):
            unique_dates = np.unique(self.averaged_data[variable].index.date)
            for date in unique_dates:
                filename = out_path + "mesonet.{0}.nc".format(date.strftime("%Y%m%d"))
                if os.access(filename, os.R_OK):
                    mode = "a"
                else:
                    mode = "w"
                ds = Dataset(filename, mode=mode)
                if mode == "w":
                    ds.createDimension("recNum", size=None)
                    ds.createDimension("timesPerDay", size=24)
                    ds.createDimension("stationNameSize", size=4)
                    c_time = ds.createVariable("creation_time", "f8")
                    c_time.assignValue(date2num(datetime.utcnow(), time_units))
                    c_time.units = time_units
                    c_time.long_name = "time at which file was created"
                    obs_times = ds.createVariable("time_nominal", "f8", ("timesPerDay", ))
                    obs_times.long_name = "observation time"
                    obs_times.units = time_units
                    valid_dates = self.averaged_data[variable].index[
                        self.averaged_data[variable].index.date == date].to_pydatetime()
                    obs_times[:] = date2num(valid_dates, time_units)
                    station_names = ds.createVariable("stationName", "S1", ("recNum", "stationNameSize"))
                    print self.averaged_data[variable].columns.values
                    station_names[:] = stringtochar(self.averaged_data[variable].columns.values.astype("S4"))
                    if station_numbers is not None:
                        site_list = ds.createVariable("site_list", "i4", ("recNum",))
                        site_list.long_name = "DICAST Identification Number"
                        site_list.reference = "DICAST Site Database"
                        site_list[:] = station_numbers
                ds_var = ds.createVariable(variable, "f4", ("recNum", "timesPerDay"))
                date_indices = self.averaged_data[variable].index.date == date
                ds_var[:] = self.averaged_data[variable].loc[date_indices, :].values.T
                ds_var.long_name = variable
                ds_var.units = self.units[variable.split("_")[0]]
                ds.close()
        return
if __name__ == "__main__":
    main()
